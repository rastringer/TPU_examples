{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82Yd9t9H9cjt"
   },
   "source": [
    "### Single-core Training FashionMNIST on Cloud TPU\n",
    "\n",
    "In this notebook we explore the simple code changes necessary to train and evaluate a model on a TPU.\n",
    "\n",
    "Some of the some of the torch_xla functions we will use include:\n",
    "* `device = xm.xla_device()` to set the device to TPU.\n",
    "* `xm.is_master_ordinal()` to make sure the current process is the master ordinal (0).\n",
    "* `xm.optimizer_step()` for evaluation each time gradients are updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: add launch as Colab and pip installs, wheel etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def trainer():\n",
    "    \n",
    "    # Random seed for initialization\n",
    "    torch.manual_seed(flags['seed'])\n",
    "    # Sets device to Cloud TPU core\n",
    "    device = xm.xla_device()\n",
    "\n",
    "    # Normalization for dataloader \n",
    "    # TorchVision models require RGB (3 x H x W) images\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "    to_rgb = transforms.Lambda(lambda image: image.convert('RGB'))\n",
    "    resize = transforms.Resize((224, 224))\n",
    "    my_transform = transforms.Compose([resize, to_rgb, transforms.ToTensor(), normalize])\n",
    "\n",
    "    # Checks if current process is the master ordinal (0)\n",
    "    # Other workers wait for master to complete download\n",
    "    if not xm.is_master_ordinal():\n",
    "        xm.rendezvous('download_only_once')\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(\n",
    "        \"/tmp/fashionmnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=my_transform)\n",
    "\n",
    "    test_dataset = datasets.FashionMNIST(\n",
    "        \"/tmp/fashionmnist\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=my_transform)\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_dataset)\n",
    "    test_sampler = torch.utils.data.RandomSampler(test_dataset)\n",
    "\n",
    "    # Dataloaders load data in batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=flags['batch_size'],\n",
    "        sampler=train_sampler\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=flags['batch_size'],\n",
    "        sampler=test_sampler\n",
    "    )\n",
    "\n",
    "    # Model, optimizer, and loss function \n",
    "    # We use a resnet18 model for the 10 classes of the FashionMNIST dataset\n",
    "    net = torchvision.models.resnet18(num_classes=10).to(device).train()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    def train():\n",
    "        train_start = time.time()\n",
    "        for data, targets in iter(train_loader):\n",
    "            # Sends data and targets to device\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # Get prediction\n",
    "            output = net(data)\n",
    "            # Loss function\n",
    "            loss = loss_fn(output, targets)\n",
    "            # Update model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # The 'barrier' here forces evaluation each time gradients are \n",
    "            # updates, keeping graphs manageable.\n",
    "            xm.optimizer_step(optimizer, barrier=True)\n",
    "\n",
    "        elapsed_train_time = time.time() - train_start\n",
    "        print(f\"Finished training. Train time was: {elapsed_train_time}\")\n",
    "\n",
    "    def test():\n",
    "        net.eval()\n",
    "        eval_start = time.time()\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            total_guesses = 0\n",
    "\n",
    "        for data, targets in iter(test_loader):\n",
    "            # Sends data and targets to device\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = net(data)\n",
    "            best_guesses = torch.argmax(output, 1)\n",
    "            # Calculate accuracy\n",
    "            num_correct += torch.eq(targets, best_guesses).sum().item()\n",
    "            total_guesses += flags['batch_size']\n",
    "\n",
    "        accuracy = 100.0 * num_correct / total_guesses\n",
    "        elapsed_eval_time = time.time() - eval_start\n",
    "        print(f\"Finished evaluation. Evaluation time was: {elapsed_eval_time}\")\n",
    "        print(f\"Guessed {num_correct} of {total_guesses} correctly for {accuracy} % accuracy.\")\n",
    "\n",
    "        return accuracy, data, targets\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    data, targets = None, None\n",
    "    \n",
    "    # Loop through epochs, calling the train and eval functions above\n",
    "    for epoch in range(flags['num_epochs']):\n",
    "        tic = time.time()\n",
    "        train()\n",
    "        xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
    "        toc = time.time()\n",
    "        accuracy, data, targets  = test()\n",
    "        # Calculate training time per epoch\n",
    "        epoch_time = round(toc-tic, 2)\n",
    "        print(f\"Epoch trained on a single TPU in {epoch_time} seconds\")\n",
    "\n",
    "    return accuracy, data, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "id": "YSKorOP29cts",
    "outputId": "79ca7502-25cf-41f3-eb35-b4f225e7f925"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "flags = {}\n",
    "flags['batch_size'] = 8\n",
    "flags['num_epochs'] = 1\n",
    "flags['seed'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we trained and evaluated a ResNet18 model on the FashionMNIST dataset.\n",
    "We used various functions from the pytorch_XLA library to run on a Cloud TPU.\n",
    "\n",
    "Try experimenting with the `batch_size` - how high can it go before the training starts to slow down?\n",
    "\n",
    "We can also experiment with different model architectures, such as EfficientNet or ConvNeXt, just by changing this line:\n",
    "\n",
    "```\n",
    "net = torchvision.models.resnet18(num_classes=10).to(device).train()\n",
    "\n",
    "```\n",
    "\n",
    "to swap `resnet18` to others listed [here](https://pytorch.org/vision/stable/models.html#classification).\n",
    "                                           \n",
    "The next notebook, `pytorch_resnet_multicore.ipynb`, shows how we can adapt our code to run on multiple TPU cores, which should\n",
    "make training more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save this notebook as a Python file, run the following commmand from a terminal in the same folder:\n",
    "```\n",
    "jupyter nbconvert --to script pytorch_resnet_singlecore.ipynb\n",
    "```\n",
    "\n",
    "We can then load the .py file into a Cloud Storage bucket, then import it via `gsutil` into the TPU VM.\n",
    "```\n",
    "gsutil cp pytorch_resnet_singlecore.py gs://<your-unique-bucket-name> .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Pytorch (Local)",
   "language": "python",
   "name": "local-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
