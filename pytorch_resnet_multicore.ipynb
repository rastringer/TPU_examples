{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "82Yd9t9H9cjt"
   },
   "source": [
    "### Multi-core Training FashionMNIST on Cloud TPU\n",
    "\n",
    "Now we are familiar with adjusting code to run on a single TPU core, we will walkthrough expanding the data loading, model training and evaluation across all eight cores of the TPU. \n",
    "Multi-core operations will predictibly be faster and able to handle larger loads (datasets, batch sizes).\n",
    "\n",
    "Beyond setting `device = xm.xla_device()`, below are some of the torch_xla functions we will use:\n",
    "* Data is only being downloaded once by a master worker by checking `xm.is_master_ordinal()`.\n",
    "* Subsets of the data are being loaded efficiently across all processes using `DistributedSampler`.\n",
    "* `xm.optimizer_step(optimizer)` to consolidates gradients between cores during training.  \n",
    "* We run the training using `xmp.spawn()` to enable replication across multiple processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def trainer():\n",
    "    \n",
    "    # Random seed for initialization\n",
    "    torch.manual_seed(flags['seed'])\n",
    "    # Sets device to Cloud TPU core\n",
    "    device = xm.xla_device()\n",
    "\n",
    "    # Normalization for dataloader \n",
    "    # TorchVision models require RGB (3 x H x W) images\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225])\n",
    "    to_rgb = transforms.Lambda(lambda image: image.convert('RGB'))\n",
    "    resize = transforms.Resize((224, 224))\n",
    "    my_transform = transforms.Compose([resize, to_rgb, transforms.ToTensor(), normalize])\n",
    "\n",
    "    # Checks if current process is the master ordinal (0)\n",
    "    # Other workers wait for master to complete download\n",
    "    if not xm.is_master_ordinal():\n",
    "        xm.rendezvous('download_only_once')\n",
    "\n",
    "    train_dataset = datasets.FashionMNIST(\n",
    "        \"/tmp/fashionmnist\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=my_transform)\n",
    "\n",
    "    test_dataset = datasets.FashionMNIST(\n",
    "        \"/tmp/fashionmnist\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=my_transform)\n",
    "\n",
    "    if xm.is_master_ordinal():\n",
    "        xm.rendezvous('download_only_once')\n",
    "\n",
    "    # DistributedSampler restricts data loading to a subset of the dataset\n",
    "    # for each process\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True)\n",
    "\n",
    "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        test_dataset,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False)\n",
    "\n",
    "    # Dataloaders load data in batches\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=flags['batch_size'],\n",
    "        sampler=train_sampler,\n",
    "        num_workers=flags['num_workers'],\n",
    "        drop_last=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=flags['batch_size'],\n",
    "        sampler=test_sampler,\n",
    "        shuffle=False,\n",
    "        num_workers=flags['num_workers'],\n",
    "        drop_last=True)\n",
    "\n",
    "    # Model, optimizer, and loss function \n",
    "    # We use a resnet18 model for the 10 classes of the \n",
    "    # FashionMNIST dataset\n",
    "    # Each process has its own copy of the model\n",
    "    net = torchvision.models.resnet18(num_classes=10).to(device).train()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    def train(loader):\n",
    "        train_start = time.time()\n",
    "        for batch_num, batch in enumerate(loader):\n",
    "            data, targets = batch\n",
    "            # Get prediction\n",
    "            output = net(data)\n",
    "            # Loss function\n",
    "            loss = loss_fn(output, targets)\n",
    "            # Update model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # xm.optimizer_step(optimizer) consolidates the gradients between cores\n",
    "            # and issues the XLA device step computation.\n",
    "            xm.optimizer_step(optimizer)\n",
    "\n",
    "        elapsed_train_time = time.time() - train_start\n",
    "        print(f\"Finished training. Train time was: {elapsed_train_time}\")\n",
    "\n",
    "    def test(loader):\n",
    "        net.eval()\n",
    "        eval_start = time.time()\n",
    "        with torch.no_grad():\n",
    "            num_correct = 0\n",
    "            total_guesses = 0\n",
    "\n",
    "        for batch_num, batch in enumerate(loader):\n",
    "            data, targets = batch\n",
    "            output = net(data)\n",
    "            best_guesses = torch.argmax(output, 1)\n",
    "            # Calculate accuracy\n",
    "            num_correct += torch.eq(targets, best_guesses).sum().item()\n",
    "            total_guesses += flags['batch_size']\n",
    "\n",
    "        accuracy = 100.0 * num_correct / total_guesses\n",
    "        elapsed_eval_time = time.time() - eval_start\n",
    "        print(f\"Finished evaluation. Evaluation time was: {elapsed_eval_time}\")\n",
    "        print(f\"Guessed {num_correct} of {total_guesses} correctly for {accuracy} % accuracy.\")\n",
    "\n",
    "        return accuracy, data, targets\n",
    "    \n",
    "    accuracy = 0.0\n",
    "    data, targets = None, None\n",
    "    \n",
    "    # Loop through epochs, calling the train and eval functions above\n",
    "    for epoch in range(flags['num_epochs']):\n",
    "        # ParallelLoader wraps a DataLoader with background data upload\n",
    "        para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
    "        train(para_train_loader)\n",
    "        xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
    "        para_test_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
    "        accuracy, data, pred, targets  = test(para_test_loader)\n",
    "\n",
    "    return accuracy, data, targets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "id": "YSKorOP29cts",
    "outputId": "79ca7502-25cf-41f3-eb35-b4f225e7f925"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "flags = {}\n",
    "flags['batch_size'] = 32\n",
    "flags['num_workers'] = 8\n",
    "flags['num_epochs'] = 1\n",
    "flags['seed'] = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *multiprocess function* runs the trainer, takes the index of each process and the flags defined above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mp_fn(index, flags):\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "    accuracy, data, targets = trainer()\n",
    "\n",
    "# Enable replication across multiple processes     \n",
    "xmp.spawn(_mp_fn, args=(flags,), nprocs=8, start_method='fork')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Pytorch (Local)",
   "language": "python",
   "name": "local-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
